
#  决策树

-  [Sklearn 运用](#id1)

-  [CART--分类回归树](#id2)


<h2 id="id2"> CART--分类回归树 </h2>

具体分为**特征选择**,**树的生成**和**剪枝**组成

-  **特征选择**

回归树通过 MSE 选择最佳特征,分类树通过基尼系数选择最佳特征作为节点



-  ***树的生成*

	-  回归树:假设已将输入空间划分为 M 个单元,并且在每个单元 Rm 上有一个固定的输出值 cm,





<h2 id="id1">Sklearn 运用</h2>


[30分钟学会用scikit-learn的基本回归方法（线性、决策树、SVM、KNN）和集成方法（随机森林，Adaboost和GBRT)](http://blog.csdn.net/u010900574/article/details/52666291)

[scikit-learn学习 - 决策树](http://www.cnblogs.com/zhaoxy/p/5054938.html)


[Decision Tree Regression](http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html#example-tree-plot-tree-regression-py)


[1.10. Decision Trees](http://scikit-learn.org/stable/modules/tree.html#tree)


[sklearn.tree.DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor)


参数设置:

-  criterion: 分类选择标准,在回归树中默认为 mse,平方根差

-  splitter:默认选择能使 mse 最小的作为一个 node,还可以选择随机

- max_features: 默认 None,针对所有的特征



























